# CIFAR10の学習・評価（Pytorch）

<p align='center'>
  <img height='300' src='./assets/cifar10.png?raw=true'>
</p>

## 目次

1. [用いた最適化手法について](#用いた最適化手法について)
    - SGD
    - Momentum SGD
    - AdaGrad
    - RMSprop
    - Adam
2. [MLPを用いた実装](#mlpを用いた実装)
    - [考察](#mlpの考察)
      - [GPUにおける実行](#gpuにおける実行)
3. [CNNを用いた実装](#cnnを用いた実装)
    - [考察](#cnnの考察)
      - [GPUにおける実行](#gpuにおける実行)
4. [実行方法](#実行方法)

---

## 用いた最適化手法について

今回，5つの最適化手法を用いて，実験・考察を行なった．  
最もシンプルなSGDから，改良を重ねたAdamを通して，精度や実行時間をみた．  
以下，それぞれの手法の簡易的説明，そして式を記す．

### SGD

<img height='50' src='./assets/sgd.png'>

勾配降下法と呼ばれる手法であり，多くはミニバッチで行われるため，確率的勾配降下法とも呼ばれる．  
パラメタの勾配を求め，それらを用いて，最適化を行う最もシンプルな方法である．  
しかし，収束の不安定性・遅さから高次元の問題で使われることはない．

### Momentum SGD

<img height='100' src='./assets/momentum_sgd.png'>

物理のモーメンタムを用いた手法．vは速度を表し，ボールが池面の傾斜を転がるように動く．  
SGDに比べて，x軸方向に受ける力が小さく，y軸方向には受ける力が大きいが，速度は安定しないので，SGDに比べてx軸方向へ早く近づくことができる．

### AdaGrad

<img height='100' src='./assets/adagrad.png'>

AdaGradは，パラメタの要素ごとに適応的に学習係数を調整しながら学習を行う手法である．  
hはこれまで経験した勾配の値を2乗和として保持することによって，学習のスケールを調整．

### RMSprop

<img height='100' src='./assets/rmsprop.png'>

RMSpropはAdaGradを改良したアルゴリズムである．  
AdaGradは学習率が0に十分近くなってしまうと，まだ坂があったとしてもほとんど更新されなくなってしまうという問題があった．  
そこで提案されたRMSpropは初期の影響がαに応じて指数的に減衰する．

### Adam
<img height='200' src='./assets/adam.png'>

RMSpropの改良版である．  
勾配に関しては，RMSpropのように，指数的減衰をするが，これに加え過去の勾配の指数関数的減数平均を保持する．

---

## MLPを用いた実装

[Pytorchのドキュメント](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)を参考に実装

<p align='center'>
  <img height='300' src='./assets/cifar10_mlp_diagram.png?raw=true'>
</p>

入力層: 100ノード  
中間層: 100ノード  
出力層: 10ノード

### MLPの考察

#### GPUにおける実行


ノード：pascal   
ノード数：1  
CUDA：10.1


##### 精度

<p align='center'>
  <img height='300' src='./assets/cifar_mlp_gpu_accuracy.png?raw=true'>
</p>

|| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10|
|--------------|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|SGD|37.752|43.66|46.556|48.798|51.02|52.94|53.654|55.63|56.71|57.768|37.79|43.9|46.09|47.63|48.93|50.11|50.35|51.2|51.0|52.24|
|Momentum SGD|45.888|51.502|55.192|57.04|58.542|59.72|61.356|62.062|61.34|64.114|44.75|48.59|50.21|51.12|51.18|51.73|51.86|51.99|50.5|51.96|
|AdaGrad|41.504|43.33|44.636|45.49|46.12|46.572|47.04|47.562|47.754|48.184|41.29|43.07|44.41|45.06|45.57|45.58|45.88|46.37|46.62|46.7|
|RMSprop|43.764|44.57|45.468|44.892|45.312|46.498|48.546|48.57|49.996|49.18|42.84|42.91|43.4|43.06|42.26|43.9|44.26|44.84|45.8|44.23|
|Adam|44.83|47.838|49.674|49.64|52.66|52.754|54.152|53.56|54.714|56.624|43.69|45.71|46.05|46.25|48.17|47.1|48.39|47.38|47.75|48.68|

MLPにおける精度はSGDが最も高い結果となった．一般的に，SGDは最もシンプルな最適化手法であるが，このような結果となった．  
考えられる要因としては，エポック数の少なさ，またパラメタの初期値の2つがある．

##### 損失


<p align='center'>
  <img height='300' src='./assets/cifar_mlp_gpu_loss.png?raw=true'>
</p>

|| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10|
|--------------|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|SGD|1.972|1.684|1.572|1.499|1.443|1.396|1.354|1.318|1.284|1.254|
|Momentum SGD|1.674|1.466|1.376|1.306|1.256|1.213|1.174|1.141|1.109|1.084|
|AdaGrad|1.79|1.658|1.613|1.584|1.563|1.547|1.533|1.521|1.51|1.501|
|RMSprop|1.769|1.672|1.634|1.596|1.578|1.554|1.533|1.514|1.495|1.478|
|Adam|1.73|1.576|1.505|1.46|1.426|1.399|1.365|1.341|1.323|1.304|

損失はSGDよりもMomentum SGDが低く，精度とは異なる結果となった．  
今回，精度は正解ラベルの確率であったため，モデルの良さを測るためにはあまりよくない可能性があることがこれからわかる．

##### 実行時間

<p align='center'>
  <img height='300' src='./assets/cifar_mlp_gpu_elapsed_time.png?raw=true'>
</p>

|| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10|
|--------------|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|SGD|24.679|47.295|69.801|92.566|115.063|137.639|160.254|183.166|206.003|229.05|
|Momentum SGD|26.268|50.377|74.396|98.535|122.955|146.85|170.841|194.76|218.757|242.775|
|AdaGrad|26.892|51.889|76.995|102.336|127.394|152.96|178.197|203.177|228.42|253.755|
|RMSprop|27.925|53.512|79.534|105.4|131.252|157.032|183.095|209.08|234.558|260.485|
|Adam|28.948|55.984|83.112|110.161|137.305|164.34|191.144|218.117|244.678|271.195|

実行時間はMnistの実験時と同様に，Adamが最も長い．これは，計算量などを考慮すると明確であり，正しい挙動であることがわかる．


---

## CNNを用いた実装

<p align='center'>
  <img height='300' src='./assets/cifar10_cnn_diagram.png?raw=true'>
</p>

入力層: 100ノード  
中間層: 100ノード  
出力層: 10ノード

### CNNの考察

#### GPUにおける実行

ノード：pascal   
ノード数：1  
CUDA：10.1


##### 精度

<p align='center'>
  <img height='300' src='./assets/cifar_cnn_gpu_accuracy.png?raw=true'>
</p>

|| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10|
|--------------|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|SGD|32.172|39.866|43.748|47.168|48.772|51.018|52.034|53.864|54.078|56.206|32.06|39.29|43.37|46.89|47.72|49.97|51.11|53.04|52.91|54.69|
|Momentum SGD|46.82|54.336|57.0|59.57|61.666|62.16|63.544|64.176|65.518|66.154|46.46|53.46|55.5|57.41|58.93|58.19|60.13|59.96|60.52|60.57|
|AdaGrad|34.18|36.542|37.894|38.994|39.838|40.1|40.714|41.268|41.584|41.986|34.09|36.96|37.96|39.0|39.54|40.43|40.9|41.3|41.38|42.31|
|RMSprop|48.892|54.312|56.822|59.808|61.51|61.784|62.718|60.582|62.964|62.254|47.64|52.87|55.1|57.47|58.75|59.18|59.5|57.65|60.08|58.26|
|Adam|50.26|54.668|57.19|60.162|61.234|61.848|63.402|64.416|64.392|65.57|49.44|53.02|54.34|56.81|57.6|57.26|58.81|59.58|58.87|60.21|

今回，Momentum SGDが最も高い精度であった．しかし，MLPとは異なり，AdamとRMSpropも6割程度の精度がでた．  
Momentum SGDやAdamはそれぞれSGDとRMSprop / AdaGradを改良した最適化手法であり，今回精度や損失からもそれぞれ改良前の最適化手法に比べ良い結果が出ている．

##### 損失

<p align='center'>
  <img height='300' src='./assets/cifar_cnn_gpu_loss.png?raw=true'>
</p>

|| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10|
|--------------|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|SGD|2.112|1.738|1.608|1.527|1.467|1.425|1.382|1.348|1.318|1.28|
|Momentum SGD|1.696|1.394|1.284|1.212|1.159|1.11|1.068|1.039|1.008|0.982|
|AdaGrad|1.978|1.797|1.742|1.715|1.692|1.673|1.657|1.641|1.633|1.625|
|RMSprop|1.631|1.39|1.293|1.232|1.193|1.156|1.142|1.123|1.113|1.106|
|Adam|1.611|1.378|1.278|1.216|1.169|1.128|1.103|1.081|1.065|1.049|


##### 実行時間

<p align='center'>
  <img height='300' src='./assets/cifar_cnn_gpu_elapsed_time.png?raw=true'>
</p>

|| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10|
|--------------|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|SGD|37.966|74.148|112.449|148.6|184.83|220.421|256.455|292.513|328.703|364.548|
|Momentum SGD|40.728|81.852|121.34|160.276|199.144|240.287|279.353|318.263|357.222|396.212|
|AdaGrad|43.166|84.581|125.973|167.612|208.567|252.03|292.647|333.514|375.178|416.016|
|RMSprop|45.029|87.875|130.904|173.751|216.469|261.428|304.135|346.902|389.531|432.562|
|Adam|48.048|95.969|142.32|188.422|234.109|282.324|327.975|373.629|419.47|465.2|

実行時間はMLPと同様にAdamが最も長い．  
MLPと異なり，実行時間がかかったのは，メモリI/Oが要因であることが考えられる．


全体的にMLPに比べ，CNNの方が良い結果となっていることがわかる．しかし，精度は6割程度しか出ていないため，今後さらに精度をあげるには以下の4点を加えることを考えている：
- エポック数を増やす：学習をさらに行える
- モデルの層を増やす：畳み込み層を増やすことで特徴抽出をさらに行う
- 学習率の調整
- 荷重減衰を加える：過学習を防ぐ

[ここで](https://zhenye-na.github.io/2018/09/28/pytorch-cnn-cifar10.html)紹介されているCNNでは85%の精度が出ているため，時間があればさらなる改造を加えてみたい．

---

## 実行方法

### main.py

それぞれ，`./cifar10`ないで行う

```
$ python main.py --net_type=$MY_NET_TYPE --optimizer_type=$MY_OPTIMIZER --device=$MY_DEVICE
```

ただし，`--net_type`は`[mlp, cnn]`，`--optimizer_type`は`[sgd, msgd, adagrad, rmsprop, adam]`，`--device`は`[cpu, gpu]`のいずれかから選ぶ

`$ python main.py --help`にて，引数の詳細が表示される．

### plot.py

`./cifar10`内で実行

`$ python plot.py --plot_type=$MY_PLOT_TYPE --net_type=$MY_NET_TYPE --device=$MY_DEVICE`

ただし，`--plot_type`は`[accuracy ,loss, elapsed_time]`のいずれか，`--device`は`[cpu, gpu]`，そして`net_type`は`[mlp, cnn]`のいずれかから選ぶ

